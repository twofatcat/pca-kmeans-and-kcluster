{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '/Users/cao.yumin/Desktop/statistic_of_ml/3/2020_assignment3_data/mnist_train.csv'\n",
    "test_file_path = '/Users/cao.yumin/Desktop/statistic_of_ml/3/2020_assignment3_data/mnist_test.csv'\n",
    "train_df = pd.read_csv(train_file_path,header=None)\n",
    "test_df = pd.read_csv(test_file_path,header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df.pop(0)\n",
    "train_features = train_df\n",
    "test_label = test_df.pop(0)\n",
    "test_features = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to_numpy()\n",
    "test_features = test_features.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_label.to_numpy()\n",
    "test_label = test_label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3)\n",
    "ax0,ax1,ax2 = axs.ravel()\n",
    "ax0.imshow(train_features[0].reshape(28,28),cmap='gray')\n",
    "ax1.imshow(train_features[1].reshape(28,28),cmap='gray')\n",
    "ax2.imshow(train_features[2].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_features=scaler.fit_transform(train_features)\n",
    "test_features=scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,3)\n",
    "ax0,ax1,ax2 = axs.ravel()\n",
    "ax0.imshow(train_features[0].reshape(28,28),cmap='gray')\n",
    "ax1.imshow(train_features[1].reshape(28,28),cmap='gray')\n",
    "ax2.imshow(train_features[2].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self,X,n): # X is the input training set(standardised)\n",
    "                            # n is the number of reduced dimensions (from 256 to 10)\n",
    "        self.X = X\n",
    "        self.n = n\n",
    "    def mean_vector(self,mean_X): # mean_X is a numpy array, [m1.T,m2.T,...]\n",
    "        mean_x = []\n",
    "        new_mean_X = mean_X.T\n",
    "        length = len(new_mean_X)\n",
    "        for i in range(length):\n",
    "            sum = np.mean(new_mean_X[i])\n",
    "            mean_x.append(sum)\n",
    "        mean_x = np.array(mean_x)\n",
    "        return mean_x # shape (1,m)\n",
    "    def covariance_matrix(self): # finding covariance_matrix of X\n",
    "        cov_matrix = np.cov(self.X)\n",
    "        return cov_matrix\n",
    "    def singular_value_decomposition(self): # singular value decomposition and get the n main components\n",
    "        m,n = self.X.shape\n",
    "        new_X = (1/(math.sqrt(n-1)))*self.X.T\n",
    "#         print('new_X',new_X.shape)\n",
    "        U, sigma, V_T = np.linalg.svd(new_X)\n",
    "#         svd = np.zeros((U.shape[0],V.shape[1]))     \n",
    "#         for i in range(0, self.n):                       \n",
    "#             svd[i, i] = b[i]\n",
    "#         print('V_T>>>,',V_T.shape)\n",
    "        V_n = V_T[:self.n]\n",
    "#         print('V_n>>>>,',V_n.shape)\n",
    "        return V_n\n",
    "    def fit(self,X):\n",
    "        V_n = self.singular_value_decomposition()\n",
    "        Y = np.matmul(V_n,X)\n",
    "#         print('Y,',Y.shape)\n",
    "#         print('----')\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "class KNN:\n",
    "    def __init__(self,train_x,train_label,test_x):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_label\n",
    "        self.test_x = test_x\n",
    "    def predict(self):\n",
    "        neigh.fit(self.train_x.T, self.train_y)\n",
    "        pred_y = neigh.predict(self.test_x.T)\n",
    "        return pred_y\n",
    "    def error(self,test_y):\n",
    "        pred_y = self.predict()\n",
    "        error_score = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if int(pred_y[i]) != int(test_y[i]):\n",
    "                error_score += 1\n",
    "        error = error_score/len(pred_y)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot_list =[] \n",
    "for i in range(10,257):\n",
    "    temp = PCA(train_features.T,i)\n",
    "    train_x = temp.fit(train_features.T)\n",
    "    test_x = temp.fit(test_features.T)\n",
    "    error = KNN(train_x,train_label,test_x).error(test_label)\n",
    "    error_plot_list.append((error,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error2 = []\n",
    "for i in [512,784]:\n",
    "    temp = PCA(train_features.T,i)\n",
    "    train_x = temp.fit(train_features.T)\n",
    "    test_x = temp.fit(test_features.T)\n",
    "    error = KNN(train_x,train_label,test_x).error(test_label)\n",
    "    error2.append((error,i))\n",
    "print(error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(error_plot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = [i[0] for i in error_plot_list]\n",
    "x_list = [i[1] for i in error_plot_list]\n",
    "plt.figure()\n",
    "plt.title('Error curve of task 2')\n",
    "plt.xlabel('reduced dimentions')\n",
    "plt.ylabel('error rate')\n",
    "plt.plot(x_list,y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA using eigenvalue and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA2:\n",
    "    def __init__(self,X,n): # X is the input training set(standardised)\n",
    "                            # n is the number of reduced dimensions (from 256 to 10)\n",
    "        self.X = X\n",
    "        self.n = n\n",
    "    def mean_vector(self,mean_X): # mean_X is a numpy array, [m1.T,m2.T,...]\n",
    "        mean_x = []\n",
    "        new_mean_X = mean_X.T\n",
    "        length = len(new_mean_X)\n",
    "        for i in range(length):\n",
    "            sum = np.mean(new_mean_X[i])\n",
    "            mean_x.append(sum)\n",
    "        mean_x = np.array(mean_x)\n",
    "        return mean_x # shape (1,m)\n",
    "    def covariance_matrix(self): # finding covariance_matrix of X\n",
    "        cov_matrix = np.cov(self.X)\n",
    "        return cov_matrix\n",
    "    def singular_value_decomposition(self): # singular value decomposition and get the n main components\n",
    "        m,n = self.X.shape\n",
    "        Rmatrix = np.dot(self.X,self.X.T)\n",
    "        Rmatrix = (1/(n-1))*Rmatrix\n",
    "        eigvalue,eigvector = np.linalg.eig(Rmatrix)\n",
    "        sorted_eigvalue_index = np.argsort(eigvalue)\n",
    "        new_eigvector = [eigvector[i] for i in sorted_eigvalue_index]\n",
    "        new_eigvector = np.array(new_eigvector)\n",
    "#         print('new_eigvector>>>,',new_eigvector.shape)\n",
    "        V_n = new_eigvector[:self.n]\n",
    "#         print('V_n>>>>,',V_n.shape)\n",
    "        return V_n\n",
    "    def fit(self,X):\n",
    "        V_n = self.singular_value_decomposition()\n",
    "        Y = np.dot(V_n,X)\n",
    "#         print('Y,',Y.shape)\n",
    "#         print('----')\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot_list2 =[] \n",
    "for i in [10*(m+1) for m in range(25)]:\n",
    "    temp = PCA2(train_features.T,i)\n",
    "    train_x = temp.fit(train_features.T)\n",
    "    test_x = temp.fit(test_features.T)\n",
    "    error = KNN(train_x,train_label,test_x).error(test_label)\n",
    "    error_plot_list2.append((error,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = [i[0] for i in error_plot_list2]\n",
    "x_list = [i[1] for i in error_plot_list2]\n",
    "plt.figure()\n",
    "plt.title('Error curve of task 2')\n",
    "plt.xlabel('reduced dimentions')\n",
    "plt.ylabel('error rate')\n",
    "plt.plot(x_list,y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(32)\n",
    "random_idx = np.random.permutation(train_features.shape[0])\n",
    "train_features[random_idx[:10]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[train_label == 1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.zeros((10,train_features.shape[1]))\n",
    "for k in range(10):\n",
    "    centroids[k, :] = np.mean(train_features[train_label == k, :], axis=0)\n",
    "centroids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k means task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 32\n",
    "np.random.RandomState(random_state)\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self,X,y,max_iter = 100,n_clusters = 10):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_iter = max_iter\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def initial_centre_points(self):\n",
    "        random_idx = np.random.permutation(self.X.shape[0])\n",
    "        centroids = self.X[random_idx[:self.n_clusters]]\n",
    "        return centroids\n",
    "             \n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "    \n",
    "    def compute_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "        return centroids\n",
    "    \n",
    "    def compute_distance(self, X, centroids):\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = np.linalg.norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "    \n",
    "    def compute_loss(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = np.linalg.norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self.initial_centre_points()\n",
    "        error_list = []\n",
    "        for i in range(self.max_iter):\n",
    "            if i%10 == 0:\n",
    "                print(i)\n",
    "            distance = self.compute_distance(X, self.centroids)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "#             print(self.labels)\n",
    "            self.new_centroids = self.compute_centroids(X, self.labels)\n",
    "            if np.all(self.new_centroids == self.centroids):\n",
    "                break\n",
    "            else:\n",
    "                self.centroids = self.new_centroids\n",
    "            error = self.compute_loss(X,self.labels,self.new_centroids)\n",
    "            error_list.append(error)\n",
    "        self.error_list = error_list\n",
    "        self.iteration = i\n",
    "        \n",
    "    def loss(self):\n",
    "        return self.error_list,self.iteration\n",
    "    \n",
    "    def accuracy(self,y):\n",
    "        count = 0\n",
    "        for i in range(len(y)):\n",
    "            if int(y[i]) == int(self.labels[i]):\n",
    "                count += 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_object = KMeans(train_features,train_label,max_iter = 100)\n",
    "k_object.fit(train_features)\n",
    "loss_list,iteration = k_object.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_list = [i for i in loss_list]\n",
    "x_list = [i for i in range(len(loss_list))]\n",
    "plt.figure()\n",
    "plt.title('loss curve of task 2')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(x_list,y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 4 -random create centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.permutation(train_features.shape[0])\n",
    "centroids = np.zeros((10,train_features.shape[1]))\n",
    "templist = [i for i in range(10)]\n",
    "tempindex = []\n",
    "j_index = []\n",
    "for i in range(10):\n",
    "    for j in random_idx:\n",
    "        if train_label[j] in templist:\n",
    "            index = templist.pop(templist.index(train_label[j]))\n",
    "#             print(templist)\n",
    "            tempindex.append(index)\n",
    "            j_index.append(j)\n",
    "            centroids[i,:] = train_features[j]\n",
    "            break\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cent = centroids[np.argsort(tempindex)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task4 create k means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class KMeans_task4:\n",
    "    def __init__(self,X,y,centroids,max_iter = 40,n_clusters = 10):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_iter = max_iter\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroids = centroids\n",
    "                 \n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "    \n",
    "    def compute_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "#         print('>>>>begin')\n",
    "        for k in range(self.n_clusters):\n",
    "#             print(X[labels == k, :])\n",
    "            if len(labels[labels == k]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "#         print('>>>end')\n",
    "        return centroids\n",
    "    \n",
    "    def compute_distance(self, X, centroids):\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = np.linalg.norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "    \n",
    "    def compute_loss(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = np.linalg.norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        error_list = []\n",
    "        for i in range(1,self.max_iter):\n",
    "            if i%20 == 0:\n",
    "                print(i)\n",
    "            distance = self.compute_distance(X, self.centroids)\n",
    "#             print('distance',distance.shape,distance)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "#             print('selflabels',self.labels.shape,self.labels)\n",
    "            self.new_centroids = self.compute_centroids(X, self.labels)\n",
    "#             print('>',self.new_centroids.shape,self.new_centroids)\n",
    "            if np.all(self.new_centroids == self.centroids):\n",
    "                break\n",
    "            else:\n",
    "                self.centroids = self.new_centroids\n",
    "#             print('----------------')\n",
    "#             error = self.compute_loss(X,self.labels,self.new_centroids)\n",
    "#             error_list.append(error)\n",
    "#         self.error_list = error_list\n",
    "#         self.iteration = i\n",
    "        \n",
    "    def calculate_percentage(self,y):\n",
    "        originallist = []\n",
    "        totallist = []\n",
    "        truelist = np.zeros(10)\n",
    "#         print(self.labels[0])\n",
    "#         print(y)\n",
    "#         print(self.labels[:20],self.labels)\n",
    "#         print(y[:20],y)\n",
    "#         print(self.labels[2],y[2],self.labels[2]==y[2],int(self.labels[2])==int(y[2]))\n",
    "        for i in range(10):\n",
    "            number = len(y[y==i])\n",
    "            originallist.append(number)\n",
    "        for i in range(10):\n",
    "            number = len(self.labels[self.labels==i])\n",
    "            totallist.append(number)\n",
    "        for i in range(len(y)):\n",
    "            if int(self.labels[i]) == int(y[i]):\n",
    "                truelist[self.labels[i]] += 1\n",
    "            else:\n",
    "                continue\n",
    "        self.originallist = originallist\n",
    "        self.totallist = totallist\n",
    "        self.truelist = truelist\n",
    "        self.percentage_list = [self.truelist[i]/self.totallist[i] for i in range(10)]\n",
    "    \n",
    "    def a(self):\n",
    "        return self.labels\n",
    "    \n",
    "    def report(self):\n",
    "        print('TP: True Positive')\n",
    "        print('FP: False Positive')\n",
    "        print('Precision = TP/(TP+FP)')\n",
    "        print('class      original     predict      true      TP      FP       precision')\n",
    "        for m in range(10):\n",
    "            a,b,c = self.originallist[m],self.totallist[m],self.truelist[m]\n",
    "            print(str(m),'\\t',a,'\\t',b,'\\t',int(c),'\\t',\\\n",
    "                 int(c),'\\t',b-int(c),'\\t',\\\n",
    "                 int(c)/(b))\n",
    "        print('\\n')\n",
    "        \n",
    "    def average_precision(self):\n",
    "        return np.mean(np.array(self.percentage_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_object2 = KMeans_task4(train_features,train_label,new_cent)\n",
    "k_object2.fit(train_features)\n",
    "k_object2.calculate_percentage(train_label)\n",
    "k_object2.report()\n",
    "k_object2.average_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_plot_list_kmeans =[] \n",
    "for i in [10*(m+1) for m in range(25)]:\n",
    "    if i%50 == 0:\n",
    "        print(i)   \n",
    "        \n",
    "    new_temp_cent = PCA(new_cent.T,i)\n",
    "    new_train_x = new_temp_cent.fit(new_cent.T)\n",
    "    new_cent2 = new_train_x.T\n",
    "    print(new_cent2.shape)\n",
    "    temp = PCA(train_features.T,i)\n",
    "    train_x = temp.fit(train_features.T)\n",
    "    test_x = temp.fit(test_features.T)\n",
    "    print(train_x.shape)\n",
    "#     print(train_x.T.shape)ii\n",
    "#     print(new_cent2.shape)\n",
    "#     print(new_cent2)ii\n",
    "    k_object3 = KMeans_task4(train_x.T,train_label,new_cent2)\n",
    "    k_object3.fit(train_x.T)\n",
    "    k_object3.calculate_percentage(train_label)\n",
    "    error = k_object3.average_precision()\n",
    "    print(error)\n",
    "    error_plot_list_kmeans.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = [i for i in error_plot_list_kmeans]\n",
    "x_list = [10*(m+1) for m in range(25)]\n",
    "plt.figure()\n",
    "plt.title('curve of task 4')\n",
    "plt.xlabel('reduced dimension')\n",
    "plt.ylabel('percentage')\n",
    "plt.plot(x_list,y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 5 add noise and doing scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = scaler.inverse_transform(train_features)\n",
    "test_features = scaler.inverse_transform(test_features)\n",
    "\n",
    "new_train_features = np.zeros((train_features.shape[0],1040))\n",
    "new_test_features = np.zeros((test_features.shape[0],1040))\n",
    "\n",
    "for i in range(train_features.shape[0]):\n",
    "    b = np.random.randint(2, size=256)\n",
    "    new_train_features[i,:] = np.concatenate((train_features[i], b), axis=None)\n",
    "for i in range(test_features.shape[0]):\n",
    "    c = np.random.randint(2, size=256)\n",
    "    new_test_features[i,:] = np.concatenate((test_features[i], c), axis=None)\n",
    "\n",
    "new_train_features = scaler.fit_transform(new_train_features)\n",
    "new_test_features = scaler.fit_transform(new_test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features =  scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot_list =[]\n",
    "l = [10,30,50,70,90,110,130,150,170,190,210,230,250]\n",
    "for i in l:\n",
    "    temp = PCA(new_train_features.T,i)\n",
    "    train_x = temp.fit(new_train_features.T)\n",
    "    test_x = temp.fit(new_test_features.T)\n",
    "    error = KNN(train_x,train_label,test_x).error(test_label)\n",
    "    error_plot_list.append((error,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = [i[0] for i in error_plot_list]\n",
    "x_list = [i[1] for i in error_plot_list]\n",
    "plt.figure()\n",
    "plt.title('Error curve of task 5')\n",
    "plt.xlabel('reduced dimentions')\n",
    "plt.ylabel('error rate')\n",
    "plt.plot(x_list,y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# build a svc without scaling the data\n",
    "\n",
    "clf = SVC(C=1.0, \n",
    "          kernel='rbf',\n",
    "          gamma='scale', \n",
    "          tol=0.01, \n",
    "          class_weight=None, \n",
    "          max_iter=-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "                     train_features, test_features, train_label.ravel(), test_label.ravel()\n",
    "\n",
    "# First scale the data, then build svc\n",
    "\n",
    "# training svc using training dataset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict test dataset\n",
    "\n",
    "y_pred_with_scale = clf.predict(X_test)\n",
    "\n",
    "print(\"Prediction report with scale:\")\n",
    "print(classification_report(y_test, y_pred_with_scale))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
